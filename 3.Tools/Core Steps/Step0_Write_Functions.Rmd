---
title: "Functions needed for true target project "
output: html_notebook
---
<!-- Functions needed for DeepTarget Project -->
```{r}
# <!-- primary target Score (DKS Score): Compute drug vs crispr correlation -  -->
correlation_bet_crispr_drug <- function(infunc_drugName){
  infunc_drugResp=onTarget$secondary_prism[infunc_drugName, common_cellLines]
  infunc_CRISPRResp=onTarget$avana_22Q2[, common_cellLines]
  correlation_df=sapply(1:nrow(infunc_CRISPRResp),function(x)
    unlist(cor.test_trimmed_v0.default(infunc_drugResp, infunc_CRISPRResp[x,])))
  correlation_df=t(correlation_df)
  row.names(correlation_df) <- all_genenames
  correlation_df
}

# <!-- secondary target Score (Secondary DKS SCore): Compute drug vs crispr correlation in cell lines where primary target is not expressed-->
correlation_bet_crispr_drug_secondary_target <- function(infunc_drugName, CellLines_withoutPrimary){
  infunc_drugResp=onTarget$secondary_prism[infunc_drugName, CellLines_withoutPrimary]
  infunc_CRISPRResp=onTarget$avana_22Q2[, CellLines_withoutPrimary]
  correlation_df=sapply(1:nrow(infunc_CRISPRResp),function(x)
    unlist(cor.test_trimmed_v0.default(infunc_drugResp, infunc_CRISPRResp[x,])))
  correlation_df=t(correlation_df)
  row.names(correlation_df) <- all_genenames
  correlation_df
}

# <!-- Mutant Specificity Score (MS Score): We reason that if a drug more specifically targets a mutant form of protein, in the cell lines with this mutant form, the similarity between viability after drug treatment and target CRISPR-KO (their DKS score) would be significantly higher than in the cell lines with WT protein. We mathematically model this dependency of the DKS score on the mutation status of the target by employing a regression between the drug and the targetâ€™s response across the cell lines, including the mutation status as an interaction term. We call this score the mutant-specificity score (this composes the second output of DeepTarget), where a positive mutant-specificity score indicates that a drug differentially targets the mutant form of the target protein more than WT and vice-versa for the negative score.-->
Mutant_Specificity_Score<- function(infunc_geneName, infunc_drugID){
  infunc_drug_matched = err_handle(drug_matched[infunc_drugID,])
  infunc_avana_matched = err_handle(avana_matched[infunc_geneName,])
  infunc_mutation_matched = err_handle(mutation_matched[infunc_geneName,])
  err_handle(summary(lm(infunc_drug_matched ~ 
                infunc_avana_matched*infunc_mutation_matched))$coefficients[4,c(1,4)])
  }




# <!-- Trimmed cor test -->
cor.test_trimmed_v0 <- function(x, ...) UseMethod("cor.test_trimmed_v0")
cor.test_trimmed_v0.default <-
  function(x, y, alternative = c("two.sided", "less", "greater"),
           method = c("pearson"), exact = NULL,
           # conf.level = 0.95,
           continuity = FALSE, ...)
  {
    alternative <- match.arg(alternative)
    method <- match.arg(method)
    DNAME <- paste(deparse(substitute(x)), "and", deparse(substitute(y)))
    
    if(length(x) != length(y))
      stop("'x' and 'y' must have the same length")
    if(!is.numeric(x)) stop("'x' must be a numeric vector")
    if(!is.numeric(y)) stop("'y' must be a numeric vector")
    OK <- complete.cases(x, y)
    x <- x[OK]
    y <- y[OK]
    n <- length(x)
    
    NVAL <- 0
    # conf.int <- FALSE
    
    if(method == "pearson") {
      if(n < 3L)
        stop("not enough finite observations")
      method <- "Pearson's product-moment correlation"
      names(NVAL) <- "correlation"
      r <- cor(x, y)
      df <- n - 2L
      ESTIMATE <- c(cor = r)
      PARAMETER <- c(df = df)
      STATISTIC <- c(t = sqrt(df) * r / sqrt(1 - r^2))
      # Do not compute confidence Int
      # if(n > 3) { ## confidence int.
      #   if(!missing(conf.level) &&
      #      (length(conf.level) != 1 || !is.finite(conf.level) ||
      #       conf.level < 0 || conf.level > 1))
      #     stop("'conf.level' must be a single number between 0 and 1")
      #   conf.int <- TRUE
      #   z <- atanh(r)
      #   sigma <- 1 / sqrt(n - 3)
      #   cint <-
      #     switch(alternative,
      #            less = c(-Inf, z + sigma * qnorm(conf.level)),
      #            greater = c(z - sigma * qnorm(conf.level), Inf),
      #            two.sided = z +
      #              c(-1, 1) * sigma * qnorm((1 + conf.level) / 2))
      #   cint <- tanh(cint)
      #   attr(cint, "conf.level") <- conf.level
      # }
      PVAL <- switch(alternative,
                     "less" = pt(STATISTIC, df),
                     "greater" = pt(STATISTIC, df, lower.tail=FALSE),
                     "two.sided" = 2 * min(pt(STATISTIC, df),
                                           pt(STATISTIC, df, lower.tail=FALSE)))
    }
    else {
      if(n < 2)
        stop("not enough finite observations")
      PARAMETER <- NULL
      TIES <- (min(length(unique(x)), length(unique(y))) < n)
      if(method == "kendall") {
        method <- "Kendall's rank correlation tau"
        names(NVAL) <- "tau"
        r <- cor(x,y, method = "kendall")
        ESTIMATE <- c(tau = r)
        
        if(!is.finite(ESTIMATE)) {  # all x or all y the same
          ESTIMATE[] <- NA
          STATISTIC <- c(T = NA)
          PVAL <- NA
        }
        else {
          if(is.null(exact))
            exact <- (n < 50)
          if(exact && !TIES) {
            q <- round((r + 1) * n * (n - 1) / 4)
            STATISTIC <- c(T = q)
            pkendall <- function(q, n) .Call(C_pKendall, q, n)
            PVAL <-
              switch(alternative,
                     "two.sided" = {
                       if(q > n * (n - 1) / 4)
                         p <- 1 - pkendall(q - 1, n)
                       else
                         p <- pkendall(q, n)
                       min(2 * p, 1)
                     },
                     "greater" = 1 - pkendall(q - 1, n),
                     "less" = pkendall(q, n))
          } else {
            xties <- table(x[duplicated(x)]) + 1
            yties <- table(y[duplicated(y)]) + 1
            T0 <- n * (n - 1)/2
            T1 <- sum(xties * (xties - 1))/2
            T2 <- sum(yties * (yties - 1))/2
            S <- r * sqrt((T0 - T1) * (T0 - T2))
            v0 <- n * (n - 1) * (2 * n + 5)
            vt <- sum(xties * (xties - 1) * (2 * xties + 5))
            vu <- sum(yties * (yties - 1) * (2 * yties + 5))
            v1 <- sum(xties * (xties - 1)) * sum(yties * (yties - 1))
            v2 <- sum(xties * (xties - 1) * (xties - 2)) *
              sum(yties * (yties - 1) * (yties - 2))
            
            var_S <- (v0 - vt - vu) / 18 +
              v1 / (2 * n * (n - 1)) +
              v2 / (9 * n * (n - 1) * (n - 2))
            
            if(exact && TIES)
              warning("Cannot compute exact p-value with ties")
            if (continuity) S <- sign(S) * (abs(S) - 1)
            STATISTIC <- c(z = S / sqrt(var_S))
            PVAL <- switch(alternative,
                           "less" = pnorm(STATISTIC),
                           "greater" = pnorm(STATISTIC, lower.tail=FALSE),
                           "two.sided" = 2 * min(pnorm(STATISTIC),
                                                 pnorm(STATISTIC, lower.tail=FALSE)))
          }
        }
      } else {
        method <- "Spearman's rank correlation rho"
        if (is.null(exact))
          exact <- TRUE
        names(NVAL) <- "rho"
        r <- cor(rank(x), rank(y))
        ESTIMATE <- c(rho = r)
        if(!is.finite(ESTIMATE)) {  # all x or all y the same
          ESTIMATE[] <- NA
          STATISTIC <- c(S = NA)
          PVAL <- NA
        }
        else {
          ## Use the test statistic S = sum(rank(x) - rank(y))^2
          ## and AS 89 for obtaining better p-values than via the
          ## simple normal approximation.
          ## In the case of no ties, S = (1-rho) * (n^3-n)/6.
          pspearman <- function(q, n, lower.tail = TRUE) {
            if(n <= 1290 && exact) # n*(n^2 - 1) does not overflow
              .Call(C_pRho, round(q) + 2*lower.tail, n, lower.tail)
            else { # for large n: asymptotic t_{n-2}
              den <- (n*(n^2-1))/6 # careful for overflow
              ## Kendall et all (1939) p. 260
              if (continuity) den <- den + 1
              r <- 1 - q/den
              pt(r / sqrt((1 - r^2)/(n-2)), df = n-2,
                 lower.tail = !lower.tail)
            }
          }
          q <- (n^3 - n) * (1 - r) / 6
          STATISTIC <- c(S = q)
          if(TIES && exact){
            exact <- FALSE
            warning("Cannot compute exact p-value with ties")
          }
          PVAL <-
            switch(alternative,
                   "two.sided" = {
                     p <- if(q > (n^3 - n) / 6)
                       pspearman(q, n, lower.tail = FALSE)
                     else
                       pspearman(q, n, lower.tail = TRUE)
                     min(2 * p, 1)
                   },
                   "greater" = pspearman(q, n, lower.tail = TRUE),
                   "less" = pspearman(q, n, lower.tail = FALSE))
        }
      }
    }
    
    RVAL <- list(
      # statistic = STATISTIC,
      # parameter = PARAMETER,
      p.value = as.numeric(PVAL),
      estimate = ESTIMATE
      # ,
      # null.value = NVAL,
      # alternative = alternative,
      # method = method,
      # data.name = DNAME
    )
    # if(conf.int)
    #   RVAL <- c(RVAL, list(conf.int = cint))
    class(RVAL) <- "htest"
    RVAL
  }
cor.test.formula <-
  function(formula, data, subset, na.action, ...)
  {
    if(missing(formula)
       || !inherits(formula, "formula")
       || length(formula) != 2L)
      stop("'formula' missing or invalid")
    m <- match.call(expand.dots = FALSE)
    if(is.matrix(eval(m$data, parent.frame())))
      m$data <- as.data.frame(data)
    ## need stats:: for non-standard evaluation
    m[[1L]] <- quote(stats::model.frame)
    m$... <- NULL
    mf <- eval(m, environment(formula))
    if(length(mf) != 2L)
      stop("invalid formula")
    DNAME <- paste(names(mf), collapse = " and ")
    names(mf) <- c("x", "y")
    y <- do.call("cor.test_trimmed_v0", c(mf, list(...)))
    y$data.name <- DNAME
    y
  }

# <!-- Functions for producing negative controld during primary target identification -->
# Create a random df unique to the input df (Control 1)
randomize_DF_with_unique_Pairs<-function(df2randomize=Positive_Set_DrugGene_Pairs){
  df2randomize_collapsed=paste(df2randomize[,1], df2randomize[,2], sep = '_')
  fixedCol=df2randomize[,2]
  shuffledCol=sample(df2randomize[,1])
  new_df=paste(shuffledCol, fixedCol, sep = '_')
  while(sum(df2randomize_collapsed==new_df)>0){
    new_df[df2randomize_collapsed==new_df]=
      paste(sample(df2randomize[,1], sum(df2randomize_collapsed==new_df)), fixedCol[df2randomize_collapsed==new_df], sep = '_')
  }
  df2return=do.call(rbind, strsplit(new_df, '_'))
  df2return=data.frame(df2return[,1], df2return[,2])
  colnames(df2return)=colnames(df2randomize)
  df2return
}
# Create a random df unique to the input df (Control 2)
randomize_DF_with_unique_Pairs_V2<-function(df2randomize=Positive_Set_DrugGene_Pairs){
  df2randomize_collapsed=paste(df2randomize[,1], df2randomize[,2], sep = '_')
  fixedCol=df2randomize[,2]
  shuffledCol=sample(rownames(corrMat), nrow(df2randomize))
  new_df=paste(shuffledCol, fixedCol, sep = '_')
  while(sum(df2randomize_collapsed==new_df)>0){
    new_df[df2randomize_collapsed==new_df]=
      paste(sample(rownames(onTarget$corrMat), sum(df2randomize_collapsed==new_df)),
            fixedCol[df2randomize_collapsed==new_df], sep = '_')
  }
  df2return=do.call(rbind, strsplit(new_df, '_'))
  df2return=data.frame(df2return[,1], df2return[,2])
  colnames(df2return)=colnames(df2randomize)
  df2return
}
```
<!-- Following are custom functions I've made to avoid repetitively writing same things again for DeepTarget; -->
```{r}
# Custom head function
myhead<-function(x){
  x[1:min(5, nrow(x)), 1:min(5, ncol(x))]
}
#Custom Error handling 
err_handle<-function(x){ tryCatch(x, error=function(e){NA}) }

# Hypergeometric test for two list of elements
hypergeometric_test_for_twolists<-function(test_list, base_list, global, lowertail=FALSE) {
  #If lowertail=FALSE - we calculate the probability for enrichment
  length(base_list)
  base_list_within_global=global[na.omit(match(base_list, global))]
  intersect_of_two_list= test_list[!is.na(match(test_list, base_list_within_global))]
  phyper(length(intersect_of_two_list)-1, #white balls in the samples
         length(base_list_within_global), #total white balls in the box
         length(global)- length(base_list_within_global), #total black balls in the box
         length(test_list), #total balls sampled
         lower.tail=lowertail) #whether you wish to calculate enrichment or depletion
}
# FDR correction using BH method
fdrcorr<-function(test_list){p.adjust(test_list, method = 'fdr')}

# Subsetting a set of columns given a matrix and a column names
colSubset<-function(mat, column_Names){
  mat[,na.omit(match(column_Names, colnames(mat)))]
}
# Subsetting a set of rows given a matrix and a column names
rowSubset<-function(mat, row_Names){
  mat[na.omit(match(row_Names, rownames(mat))),]
}
# Subsetting a vector given names
vectorSubset<-function(vec, Names){
  vec[!is.na(match(names(vec), Names))]
}

# Install or load a given package;
installORload<-function(packages){
  package.check <- lapply(packages, FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
      library(x, character.only = TRUE)
    }
  })
}

# Install or load bioconductor;
installORload.bioc<-function(packages){
  package.check <- lapply(packages, FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      BiocManager::install(x)
      library(x, character.only = TRUE)
    }
  })
}

# Identify top X percent values
topXPercentValue<-function(vec, X_percentile=95){
  vec=na.omit(vec)
  len=length(vec)
  vec=sort(vec)
  vec[ceiling(len*(X_percentile/100))]
}

# Scale 0-1 to a vector
range01 <- function(x){
  # Chossing 95% and 5% percentile as thresholds for outliers
  substitute_of_Min=topXPercentValue(vec=x, 
                                     X_percentile=5)
  substitute_of_Max=topXPercentValue(vec=x, 
                                     X_percentile=95)
  x_scaled=(x-substitute_of_Min)/(substitute_of_Max-substitute_of_Min)
  x_scaled[x_scaled<0]=0
  x_scaled[x_scaled>1]=1
  x_scaled
}

# Converting a factor to numeric directly
factor2numeric<-function(x){
  as.numeric(as.character(x))
}

# My cor test
mycorTest<-function(x, y){
  unlist(cor.test(x, y)[c(3, 4)])
}

# Striping all the non-alpha numeric from a string to match; This removes a lot of noises in names
stripall2match<-function(x){
  # Strip all non-char and non-numeric and make lower case
  # this is primarily to facilitate inconsistent naming
  tolower(gsub('[^A-z0-9]','',x) )
}

# Opposite of %in%
'%!in%' <- function(x,y)!('%in%'(x,y))

# strsplit; but for a list
strsplit_customv0 <- function(infunc_list=pred_viab$cellLines_mapping$cellLine_ID,
                              infunc_split_by='_',
                              retreving_onject_id=1){
  sapply(strsplit(infunc_list, split = infunc_split_by), function(x) x[retreving_onject_id])
}

#Max of a column
colMax <- function (colData) {
  apply(colData, MARGIN=c(2), max)
}

#median of a column
colMedian <- function (colData) {
  apply(colData, MARGIN=c(2), median)
}
#Max of a row
rowMax <- function (colData) {
  apply(colData, MARGIN=c(1), max)
}

#Min of a column
rowMin <- function (colData) {
  apply(colData, MARGIN=c(1), min)
}

#product of a row
rowProd <- function (colData) {
  apply(colData, 1, prod)
}

# Match after striping both the strings
match_stripall <- function(x, y){
  x_striped=tolower(gsub('[^A-z0-9]','',x) )
  y_striped=tolower(gsub('[^A-z0-9]','',y) )
  match(x_striped, y_striped)
}

```

