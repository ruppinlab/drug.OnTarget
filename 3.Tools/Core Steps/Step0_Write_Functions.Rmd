---
title: "Functions needed for true target project "
output: html_notebook
---
<!-- Functions needed for DeepTarget Project -->
<!-- The below code defines a function called correlation_bet_crispr_drug(). This function takes in a drug name as input and calculates the Pearson's correlation coefficient between the drug response and CRISPR response for each gene. -->
<!-- The function first extracts the drug response data for the specified drug and the common cell lines using the onTarget data object. It then extracts the CRISPR response data for the common cell lines. -->
<!-- Next, the function uses the sapply() function to apply the cor.test_trimmed_v0.default() function on each row of the CRISPR response data. This calculates the Pearson's correlation coefficient between the drug response and CRISPR response for each gene. The results are saved in a data frame, which is transposed and the row names are set to the names of all genes. Finally, the data frame is returned as the output of the correlation_bet_crispr_drug() function. -->
```{r}
# <!-- primary target Score (DKS Score): Compute drug vs crispr correlation -  -->
correlation_bet_crispr_drug <- function(infunc_drugName){
  infunc_drugResp=onTarget$secondary_prism[infunc_drugName, common_cellLines]
  infunc_CRISPRResp=onTarget$avana_22Q2[, common_cellLines]
  correlation_df=sapply(1:nrow(infunc_CRISPRResp),function(x)
    unlist(cor.test_trimmed_v0.default(infunc_drugResp, infunc_CRISPRResp[x,])))
  correlation_df=t(correlation_df)
  row.names(correlation_df) <- all_genenames
  correlation_df
}
```
<!-- The above code defines a function called correlation_bet_crispr_drug_secondary_target(). This function is similar to the correlation_bet_crispr_drug() function, but it calculates the Pearson's correlation coefficient between the drug response and CRISPR response for cell lines where the primary target is not expressed. -->

<!-- The function takes in a drug name and a list of cell lines where the primary target is not expressed as input. It then extracts the drug response data for the specified drug and the specified cell lines using the onTarget data object. It then extracts the CRISPR response data for the specified cell lines. -->
```{r}
# <!-- secondary target Score (Secondary DKS SCore): Compute drug vs crispr correlation in cell lines where primary target is not expressed-->
correlation_bet_crispr_drug_secondary_target <- function(infunc_drugName, CellLines_withoutPrimary){
  infunc_drugResp=onTarget$secondary_prism[infunc_drugName, CellLines_withoutPrimary]
  infunc_CRISPRResp=onTarget$avana_22Q2[, CellLines_withoutPrimary]
  correlation_df=sapply(1:nrow(infunc_CRISPRResp),function(x)
    unlist(cor.test_trimmed_v0.default(infunc_drugResp, infunc_CRISPRResp[x,])))
  correlation_df=t(correlation_df)
  row.names(correlation_df) <- all_genenames
  correlation_df
}
```
<!-- The below code defines a function called Mutant_Specificity_Score(). This function calculates the mutant specificity score for a given gene and drug. -->

<!-- The function takes in a gene name and drug ID as input. It then extracts the drug response data, CRISPR response data, and mutation status data for the specified gene and drug using the drug_matched, avana_matched, and mutation_matched data objects. -->

<!-- Next, the function uses the lm() function to fit a linear regression model with the drug response as the dependent variable and the CRISPR response and mutation status as independent variables. The mutation status is included as an interaction term in the model. The function then extracts the coefficient and p-value for the interaction term from the regression summary and returns them as the output of the Mutant_Specificity_Score() function. -->
```{r}
# <!-- Mutant Specificity Score (MS Score): We reason that if a drug more specifically targets a mutant form of protein, in the cell lines with this mutant form, the similarity between viability after drug treatment and target CRISPR-KO (their DKS score) would be significantly higher than in the cell lines with WT protein. We mathematically model this dependency of the DKS score on the mutation status of the target by employing a regression between the drug and the targetâ€™s response across the cell lines, including the mutation status as an interaction term. We call this score the mutant-specificity score (this composes the second output of DeepTarget), where a positive mutant-specificity score indicates that a drug differentially targets the mutant form of the target protein more than WT and vice-versa for the negative score.-->
Mutant_Specificity_Score<- function(infunc_geneName, infunc_drugID){
  infunc_drug_matched = err_handle(drug_matched[infunc_drugID,])
  infunc_avana_matched = err_handle(avana_matched[infunc_geneName,])
  infunc_mutation_matched = err_handle(mutation_matched[infunc_geneName,])
  err_handle(summary(lm(infunc_drug_matched ~ 
                infunc_avana_matched*infunc_mutation_matched))$coefficients[4,c(1,4)])
  }
```

<!-- Trimmed cor test that is faster than standard cor.test -->
```{r}
cor.test_trimmed_v0 <- function(x, ...) UseMethod("cor.test_trimmed_v0")
cor.test_trimmed_v0.default <-
  function(x, y, alternative = c("two.sided", "less", "greater"),
           method = c("pearson"), exact = NULL,
           # conf.level = 0.95,
           continuity = FALSE, ...)
  {
    alternative <- match.arg(alternative)
    method <- match.arg(method)
    DNAME <- paste(deparse(substitute(x)), "and", deparse(substitute(y)))
    
    if(length(x) != length(y))
      stop("'x' and 'y' must have the same length")
    if(!is.numeric(x)) stop("'x' must be a numeric vector")
    if(!is.numeric(y)) stop("'y' must be a numeric vector")
    OK <- complete.cases(x, y)
    x <- x[OK]
    y <- y[OK]
    n <- length(x)
    
    NVAL <- 0
    # conf.int <- FALSE
    
    if(method == "pearson") {
      if(n < 3L)
        stop("not enough finite observations")
      method <- "Pearson's product-moment correlation"
      names(NVAL) <- "correlation"
      r <- cor(x, y)
      df <- n - 2L
      ESTIMATE <- c(cor = r)
      PARAMETER <- c(df = df)
      STATISTIC <- c(t = sqrt(df) * r / sqrt(1 - r^2))
      # Do not compute confidence Int
      # if(n > 3) { ## confidence int.
      #   if(!missing(conf.level) &&
      #      (length(conf.level) != 1 || !is.finite(conf.level) ||
      #       conf.level < 0 || conf.level > 1))
      #     stop("'conf.level' must be a single number between 0 and 1")
      #   conf.int <- TRUE
      #   z <- atanh(r)
      #   sigma <- 1 / sqrt(n - 3)
      #   cint <-
      #     switch(alternative,
      #            less = c(-Inf, z + sigma * qnorm(conf.level)),
      #            greater = c(z - sigma * qnorm(conf.level), Inf),
      #            two.sided = z +
      #              c(-1, 1) * sigma * qnorm((1 + conf.level) / 2))
      #   cint <- tanh(cint)
      #   attr(cint, "conf.level") <- conf.level
      # }
      PVAL <- switch(alternative,
                     "less" = pt(STATISTIC, df),
                     "greater" = pt(STATISTIC, df, lower.tail=FALSE),
                     "two.sided" = 2 * min(pt(STATISTIC, df),
                                           pt(STATISTIC, df, lower.tail=FALSE)))
    }
    else {
      if(n < 2)
        stop("not enough finite observations")
      PARAMETER <- NULL
      TIES <- (min(length(unique(x)), length(unique(y))) < n)
      if(method == "kendall") {
        method <- "Kendall's rank correlation tau"
        names(NVAL) <- "tau"
        r <- cor(x,y, method = "kendall")
        ESTIMATE <- c(tau = r)
        
        if(!is.finite(ESTIMATE)) {  # all x or all y the same
          ESTIMATE[] <- NA
          STATISTIC <- c(T = NA)
          PVAL <- NA
        }
        else {
          if(is.null(exact))
            exact <- (n < 50)
          if(exact && !TIES) {
            q <- round((r + 1) * n * (n - 1) / 4)
            STATISTIC <- c(T = q)
            pkendall <- function(q, n) .Call(C_pKendall, q, n)
            PVAL <-
              switch(alternative,
                     "two.sided" = {
                       if(q > n * (n - 1) / 4)
                         p <- 1 - pkendall(q - 1, n)
                       else
                         p <- pkendall(q, n)
                       min(2 * p, 1)
                     },
                     "greater" = 1 - pkendall(q - 1, n),
                     "less" = pkendall(q, n))
          } else {
            xties <- table(x[duplicated(x)]) + 1
            yties <- table(y[duplicated(y)]) + 1
            T0 <- n * (n - 1)/2
            T1 <- sum(xties * (xties - 1))/2
            T2 <- sum(yties * (yties - 1))/2
            S <- r * sqrt((T0 - T1) * (T0 - T2))
            v0 <- n * (n - 1) * (2 * n + 5)
            vt <- sum(xties * (xties - 1) * (2 * xties + 5))
            vu <- sum(yties * (yties - 1) * (2 * yties + 5))
            v1 <- sum(xties * (xties - 1)) * sum(yties * (yties - 1))
            v2 <- sum(xties * (xties - 1) * (xties - 2)) *
              sum(yties * (yties - 1) * (yties - 2))
            
            var_S <- (v0 - vt - vu) / 18 +
              v1 / (2 * n * (n - 1)) +
              v2 / (9 * n * (n - 1) * (n - 2))
            
            if(exact && TIES)
              warning("Cannot compute exact p-value with ties")
            if (continuity) S <- sign(S) * (abs(S) - 1)
            STATISTIC <- c(z = S / sqrt(var_S))
            PVAL <- switch(alternative,
                           "less" = pnorm(STATISTIC),
                           "greater" = pnorm(STATISTIC, lower.tail=FALSE),
                           "two.sided" = 2 * min(pnorm(STATISTIC),
                                                 pnorm(STATISTIC, lower.tail=FALSE)))
          }
        }
      } else {
        method <- "Spearman's rank correlation rho"
        if (is.null(exact))
          exact <- TRUE
        names(NVAL) <- "rho"
        r <- cor(rank(x), rank(y))
        ESTIMATE <- c(rho = r)
        if(!is.finite(ESTIMATE)) {  # all x or all y the same
          ESTIMATE[] <- NA
          STATISTIC <- c(S = NA)
          PVAL <- NA
        }
        else {
          ## Use the test statistic S = sum(rank(x) - rank(y))^2
          ## and AS 89 for obtaining better p-values than via the
          ## simple normal approximation.
          ## In the case of no ties, S = (1-rho) * (n^3-n)/6.
          pspearman <- function(q, n, lower.tail = TRUE) {
            if(n <= 1290 && exact) # n*(n^2 - 1) does not overflow
              .Call(C_pRho, round(q) + 2*lower.tail, n, lower.tail)
            else { # for large n: asymptotic t_{n-2}
              den <- (n*(n^2-1))/6 # careful for overflow
              ## Kendall et all (1939) p. 260
              if (continuity) den <- den + 1
              r <- 1 - q/den
              pt(r / sqrt((1 - r^2)/(n-2)), df = n-2,
                 lower.tail = !lower.tail)
            }
          }
          q <- (n^3 - n) * (1 - r) / 6
          STATISTIC <- c(S = q)
          if(TIES && exact){
            exact <- FALSE
            warning("Cannot compute exact p-value with ties")
          }
          PVAL <-
            switch(alternative,
                   "two.sided" = {
                     p <- if(q > (n^3 - n) / 6)
                       pspearman(q, n, lower.tail = FALSE)
                     else
                       pspearman(q, n, lower.tail = TRUE)
                     min(2 * p, 1)
                   },
                   "greater" = pspearman(q, n, lower.tail = TRUE),
                   "less" = pspearman(q, n, lower.tail = FALSE))
        }
      }
    }
    
    RVAL <- list(
      # statistic = STATISTIC,
      # parameter = PARAMETER,
      p.value = as.numeric(PVAL),
      estimate = ESTIMATE
      # ,
      # null.value = NVAL,
      # alternative = alternative,
      # method = method,
      # data.name = DNAME
    )
    # if(conf.int)
    #   RVAL <- c(RVAL, list(conf.int = cint))
    class(RVAL) <- "htest"
    RVAL
  }
cor.test.formula <-
  function(formula, data, subset, na.action, ...)
  {
    if(missing(formula)
       || !inherits(formula, "formula")
       || length(formula) != 2L)
      stop("'formula' missing or invalid")
    m <- match.call(expand.dots = FALSE)
    if(is.matrix(eval(m$data, parent.frame())))
      m$data <- as.data.frame(data)
    ## need stats:: for non-standard evaluation
    m[[1L]] <- quote(stats::model.frame)
    m$... <- NULL
    mf <- eval(m, environment(formula))
    if(length(mf) != 2L)
      stop("invalid formula")
    DNAME <- paste(names(mf), collapse = " and ")
    names(mf) <- c("x", "y")
    y <- do.call("cor.test_trimmed_v0", c(mf, list(...)))
    y$data.name <- DNAME
    y
  }
```

<!-- This function creates a random data frame with unique pairs of drug and gene names from an input data frame.The function takes in a data frame with two columns, DrugName and GeneName, as input and creates a vector of concatenated strings of drug and gene names. It then shuffles the DrugName column and checks if any of the new drug-gene pairs are present in the original data frame. If any of the new pairs are present, they are replaced with new random drug-gene pairs until all pairs are unique. The function then splits the concatenated strings into separate DrugName and GeneName columns and returns a data frame with the new pairs. The columns of the returned data frame are named the same as the input data frame. -->
```{r}
# <!-- Functions for producing negative controld during primary target identification -->
# Create a random df unique to the input df (Control 1)
randomize_DF_with_unique_Pairs<-function(df2randomize=Positive_Set_DrugGene_Pairs){
  df2randomize_collapsed=paste(df2randomize[,1], df2randomize[,2], sep = '_')
  fixedCol=df2randomize[,2]
  shuffledCol=sample(df2randomize[,1])
  new_df=paste(shuffledCol, fixedCol, sep = '_')
  while(sum(df2randomize_collapsed==new_df)>0){
    new_df[df2randomize_collapsed==new_df]=
      paste(sample(df2randomize[,1], sum(df2randomize_collapsed==new_df)), fixedCol[df2randomize_collapsed==new_df], sep = '_')
  }
  df2return=do.call(rbind, strsplit(new_df, '_'))
  df2return=data.frame(df2return[,1], df2return[,2])
  colnames(df2return)=colnames(df2randomize)
  df2return
}
```
<!-- The above code defines a function called randomize_DF_with_unique_Pairs_V2(). This function is similar to the randomize_DF_with_unique_Pairs() function, but it creates a data frame with drug-gene pairs that are not present in the input data frame, rather than just being unique. The function takes in a data frame with two columns, DrugName and GeneName, as input and creates a vector of concatenated strings of drug and gene names. It then shuffles the DrugName column and checks if any of the new drug-gene pairs are present in the original data frame. If any of the new pairs are present, they are replaced with new random drug-gene pairs until all pairs are not present in the original data frame. -->
```{r}
# Create a random df unique to the input df (Control 2)
randomize_DF_with_unique_Pairs_V2<-function(df2randomize=Positive_Set_DrugGene_Pairs){
  df2randomize_collapsed=paste(df2randomize[,1], df2randomize[,2], sep = '_')
  fixedCol=df2randomize[,2]
  shuffledCol=sample(rownames(corrMat), nrow(df2randomize))
  new_df=paste(shuffledCol, fixedCol, sep = '_')
  while(sum(df2randomize_collapsed==new_df)>0){
    new_df[df2randomize_collapsed==new_df]=
      paste(sample(rownames(onTarget$corrMat), sum(df2randomize_collapsed==new_df)),
            fixedCol[df2randomize_collapsed==new_df], sep = '_')
  }
  df2return=do.call(rbind, strsplit(new_df, '_'))
  df2return=data.frame(df2return[,1], df2return[,2])
  colnames(df2return)=colnames(df2randomize)
  df2return
}
```
<!-- Starting are custom functions I've made to avoid repetitively writing same things again for DeepTarget; -->
```{r}
# This function is a simple wrapper for the head() function that returns the first 5 rows and columns of a data frame or matrix. The function takes in a data frame or matrix as input and uses the head() function to return the first 5 rows and columns of the input. If the input has fewer than 5 rows or columns, the entire input is returned. This function can be useful for quickly checking the first few rows and columns of a large data frame or matrix.
myhead<-function(x){
  x[1:min(5, nrow(x)), 1:min(5, ncol(x))]
}
```


```{r}
#Custom Error handling: The err_handle() function is a simple function that tries to evaluate an expression x, and if an error occurs it returns NA instead. This is known as error handling, and it allows the code to continue running even if an error is encountered.
err_handle<-function(x){ tryCatch(x, error=function(e){NA}) }
```


```{r}
# Hypergeometric test for two list of elements: This function performs a hypergeometric test to determine whether the test_list and base_list are statistically different from each other. It does this by calculating the probability of observing the overlap between the two lists, given the total number of items in the population (i.e., global). If this probability is low, it indicates that the lists are likely to be different from each other. The lowertail argument determines whether the probability is calculated for enrichment or depletion.
hypergeometric_test_for_twolists <- function(test_list, base_list, global, lowertail = FALSE) {
  
  # Calculate the length of the base list
  length(base_list)
  
  # Create a new list containing only the items in the base list that are also in the global list
  base_list_within_global <- global[na.omit(match(base_list, global))]
  
  # Create a new list containing only the items in the test list that are also in the base list within the global list
  intersect_of_two_list <- test_list[!is.na(match(test_list, base_list_within_global))]
  
  # Use the phyper() function to calculate the probability of observing the overlap between the two lists
  phyper(length(intersect_of_two_list) - 1, # Number of "white balls" (items in the overlap) in the sample
         length(base_list_within_global),   # Total number of "white balls" (items in the base list) in the box
         length(global) - length(base_list_within_global), # Total number of "black balls" (items not in the base list) in the box
         length(test_list),                 # Total number of balls sampled
         lower.tail = lowertail)            # Whether to calculate enrichment or depletion
}

```


```{r}
# The fdrcorr() function is a simple function that adjusts the p-values in a list of tests using the false discovery rate (FDR) method. The FDR is a statistical method for controlling the rate of false positives in hypothesis testing. It is often used in multiple testing, where many tests are performed on the same data.
# 
# The fdrcorr() function takes in a list of p-values as its only argument, and then applies the FDR method to adjust the p-values in the list. The adjusted p-values are then returned by the function.
fdrcorr <- function(test_list) {
    # Use the p.adjust function from the stats package in R to adjust the p-values
    # in the test_list using the false discovery rate method.
    p.adjust(test_list, method = 'fdr')
}
```
<!-- Below, I have described some simple custom functions to avoid repeating them multiple times-->
```{r}
# Subsetting a set of columns given a matrix and a column names
colSubset<-function(mat, column_Names){
  mat[,na.omit(match(column_Names, colnames(mat)))]
}
# Subsetting a set of rows given a matrix and a column names
rowSubset<-function(mat, row_Names){
  mat[na.omit(match(row_Names, rownames(mat))),]
}
# Subsetting a vector given names
vectorSubset<-function(vec, Names){
  vec[!is.na(match(names(vec), Names))]
}

# Install or load a given package;
installORload<-function(packages){
  package.check <- lapply(packages, FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
      library(x, character.only = TRUE)
    }
  })
}

# Install or load bioconductor;
installORload.bioc<-function(packages){
  package.check <- lapply(packages, FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      BiocManager::install(x)
      library(x, character.only = TRUE)
    }
  })
}

# Identify top X percent values
topXPercentValue<-function(vec, X_percentile=95){
  vec=na.omit(vec)
  len=length(vec)
  vec=sort(vec)
  vec[ceiling(len*(X_percentile/100))]
}

# Scale 0-1 to a vector
range01 <- function(x){
  # Chossing 95% and 5% percentile as thresholds for outliers
  substitute_of_Min=topXPercentValue(vec=x, 
                                     X_percentile=5)
  substitute_of_Max=topXPercentValue(vec=x, 
                                     X_percentile=95)
  x_scaled=(x-substitute_of_Min)/(substitute_of_Max-substitute_of_Min)
  x_scaled[x_scaled<0]=0
  x_scaled[x_scaled>1]=1
  x_scaled
}

# Converting a factor to numeric directly
factor2numeric<-function(x){
  as.numeric(as.character(x))
}

# My cor test
mycorTest<-function(x, y){
  unlist(cor.test(x, y)[c(3, 4)])
}

# Striping all the non-alpha numeric from a string to match; This removes a lot of noises in names
stripall2match<-function(x){
  # Strip all non-char and non-numeric and make lower case
  # this is primarily to facilitate inconsistent naming
  tolower(gsub('[^A-z0-9]','',x) )
}

# Opposite of %in%
'%!in%' <- function(x,y)!('%in%'(x,y))

# strsplit; but for a list
strsplit_customv0 <- function(infunc_list=pred_viab$cellLines_mapping$cellLine_ID,
                              infunc_split_by='_',
                              retreving_onject_id=1){
  sapply(strsplit(infunc_list, split = infunc_split_by), function(x) x[retreving_onject_id])
}

#Max of a column
colMax <- function (colData) {
  apply(colData, MARGIN=c(2), max)
}

#median of a column
colMedian <- function (colData) {
  apply(colData, MARGIN=c(2), median)
}
#Max of a row
rowMax <- function (colData) {
  apply(colData, MARGIN=c(1), max)
}

#Min of a column
rowMin <- function (colData) {
  apply(colData, MARGIN=c(1), min)
}

#product of a row
rowProd <- function (colData) {
  apply(colData, 1, prod)
}

# Match after striping both the strings
match_stripall <- function(x, y){
  x_striped=tolower(gsub('[^A-z0-9]','',x) )
  y_striped=tolower(gsub('[^A-z0-9]','',y) )
  match(x_striped, y_striped)
}
```